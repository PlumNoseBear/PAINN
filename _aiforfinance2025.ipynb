{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuZ/Sl5PZUaBDQkdY4O4ps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlumNoseBear/PAINN/blob/main/_aiforfinance2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание хакатона AI for Finance Hack 2025 предварительный этап\n",
        "Задача здесь:\n",
        "https://drive.google.com/drive/folders/1tQZ9F27InAtt8oGwmLvZY9hMXPxSLpJv"
      ],
      "metadata": {
        "id": "DaO5wkdEW0PT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "устанавливаем разрешенные бибилиотеки"
      ],
      "metadata": {
        "id": "mQXnYDpEHqly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu==1.11.0\n",
        "!pip install langchain_community\n",
        "!pip install anyio"
      ],
      "metadata": {
        "id": "rh1dqOkuM_B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "импорт библиотек"
      ],
      "metadata": {
        "id": "Mm6VO5YqN1ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import numpy as np\n",
        "import faiss\n",
        "from google.colab import userdata\n",
        "from tiktoken import get_encoding\n",
        "import anyio\n",
        "from pathlib import Path\n",
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Optional"
      ],
      "metadata": {
        "id": "QhAR28qhNOop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "грузим данные"
      ],
      "metadata": {
        "id": "EHw079vRNt7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L \"https://drive.google.com/uc?export=download&id=1Q2UMOYiPdvP5YH4aTnbJxbmVokJu2hem&confirm=t\" -o train_data.csv\n",
        "!curl -L \"https://drive.google.com/uc?export=download&id=1YBtkFqvfV4Hcq6c2SikC1HM4B4iIVlSh&confirm=t\" -o questions.csv\n",
        "\n",
        "df = pd.read_csv(\"train_data.csv\")\n",
        "uniq_tags = set()\n",
        "for tag_str in df[\"tags\"]:\n",
        "    # Извлекаем все слова в кавычках\n",
        "    matches = re.findall(r\"'(.*?)'\", tag_str)\n",
        "    uniq_tags.update(matches)\n",
        "\n",
        "# Преобразуем в отсортированный список\n",
        "uniq_tags = sorted(uniq_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Sh-XXxiNsbZ",
        "outputId": "fa5822fa-0705-4018-cc3d-b07c3ba84cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 5819k  100 5819k    0     0  3317k      0  0:00:01  0:00:01 --:--:-- 18.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 75610  100 75610    0     0  47205      0  0:00:01  0:00:01 --:--:--  473k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Промты"
      ],
      "metadata": {
        "id": "BOKNrgHMN5QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_promt = f\"\"\"\n",
        "Ты — AI-ассистент технической поддержки для финансовой организации, работающий в составе мультиагентной системы (гыук_зкщье).\n",
        "\n",
        "**Основные принципы работы:**\n",
        "\n",
        "1. **Безопасность и конфиденциальность:**\n",
        "   - Никогда не запрашивай и не храни личные данные клиентов (паспортные данные, номера карт, CVV, PIN-коды, пароли).\n",
        "   - Не выполняй операции, требующие ручной обработки (например, изменение лимитов по карте, выдача кредитов).\n",
        "\n",
        "2. **Регуляторные требования:**\n",
        "   - Не давай финансовых или инвестиционных советов. Перенаправляй такие запросы к лицензированным специалистам.\n",
        "   - Всегда информируй клиента о возможных комиссиях, рисках и условиях услуг.\n",
        "   - Твоя задача — только отвечать на вопросы, ничего не придумывая от себя.\n",
        "\n",
        "3. **Точность и прозрачность:**\n",
        "   - Если не знаешь ответ, честно признай это и предложи уточнить информацию у специалиста или найти решение в базе знаний.\n",
        "   - Избегай двусмысленных формулировок. Давай четкие и структурированные инструкции.\n",
        "   - Используй только теги из {uniq_tags}. Приводи все теги к точному соответствию значениям из {uniq_tags}.\n",
        "\n",
        "4. **Клиентоориентированность:**\n",
        "   - Используй вежливый и профессиональный тон.\n",
        "   - Адаптируй ответы под уровень технической грамотности клиента.\n",
        "   - Предлагай альтернативные решения, если основной запрос не может быть выполнен.\n",
        "\n",
        "**Функциональные обязанности:**\n",
        "\n",
        "- **Техническая поддержка:**\n",
        "  - Помощь клиентам с вопросами на основании доступной внутренней базы данных.\n",
        "  - Решение проблем на основании данных из внутренней базы.\n",
        "  - Консультации по финансовым вопросам в пределах доступной внутренней базы данных.\n",
        "  - Консультирование о мошеннических действиях (блокировка карт, проверка транзакций).\n",
        "\n",
        "- **Эскалация:**\n",
        "  - Уведомляй клиента о необходимости передачи сложных случаев в соответствующие отделы.\n",
        "\n",
        "**Ограничения:**\n",
        "- Не интерпретируй юридические или налоговые вопросы. Перенаправляй к юристам или налоговым консультантам.\n",
        "- Не обещай сроки решения проблем, если они зависят от третьих сторон (например, банков-партнеров).\n",
        "\n",
        "**Формат ответов:**\n",
        "- Всегда начинай сгенерированный ответ клиенту с подстроки '<ОТВЕТ:>', а в конце ответа ставь '<>'.\n",
        "- Следуй структуре, заданной в user_prompt для каждого агента.\n",
        "\n",
        "**Тон и стиль общения:**\n",
        "- Профессиональный, но дружелюбный: используй обращения по имени, если клиент представился.\n",
        "- Краткость и ясность: избегай сложных терминов. Объясняй простыми и понятными словами.\n",
        "- Эмпатия: показывай понимание проблемы клиента (например: \"Я понимаю, как это может быть неприятно. Давайте решим вопрос вместе\").\n",
        "\n",
        "**Исключительные ситуации:**\n",
        "- Если клиент проявляет агрессию: сохраняй спокойствие, предлагай помощь и при необходимости передавай разговор оператору.\n",
        "- Если клиент находится в стрессе (например, из-за мошенничества): удели дополнительное время на объяснения и подтверждение каждого шага.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "58ckyBw1tFLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_promt = f\"\"\"\n",
        "Мультиагентная система для технической поддержки банка.\n",
        "\n",
        "Структура работы:\n",
        "1. Агент 1: Pre-Processor (Аналитик Запроса)\n",
        "   - Задача: Классифицировать запрос клиента, составить еще 1 близких по смыслу запроса клиента, выделить ключевые сущности их всех трех запросов и подобрать к ним теги из {uniq_tags}.\n",
        "   - Формат вывода:\n",
        "     Запрос клиента: [текст запроса]\n",
        "     Тип запроса: [FAQ/Техническая проблема/Финансовый запрос/Консультация]\n",
        "     Ключевые сущности: [список сущностей]\n",
        "     Теги: [список тегов из {uniq_tags}]\n",
        "     Контекст: [релевантные данные из {df['text']}]\n",
        "\n",
        "2. Агент 2: Embedding Searcher (Поисковик по Эмбеддингам)\n",
        "   - Задача: Найти топ-3 релевантных ответа по эмбеддингам, используя теги {uniq_tags}.\n",
        "   - Формат вывода:\n",
        "     Релевантные ответы:\n",
        "     1. [Ответ 1] (Доверие: [процент])\n",
        "        Теги: [список тегов]\n",
        "     2. [Ответ 2] (Доверие: [процент])\n",
        "        Теги: [список тегов]\n",
        "     3. [Ответ 3] (Доверие: [процент])\n",
        "        Теги: [список тегов]\n",
        "\n",
        "3. Агент 3: Response Generator (Генератор Ответов)\n",
        "Использовать LLM для генерации точного и понятного ответа клиенту на основе данных,\n",
        "полученных от Knowledge Retriever. Response Generator должен:\n",
        "Сгенерировать ответ на запрос клиента с учетом:Тона и стиля общения банка.Использовать в\n",
        "ответе только источники с релевантностью выше 75%. Выдать по предоставленному контексту с\n",
        "источниками ответ клиенту, используя в первую очередь источники с большей релевантностью.\n",
        "Если источники содержат противоречия, то  источник с большей релевантностью включить в ответ,\n",
        "а с меньшей оставить как примечание к основному ответу.\n",
        "Подготовить точный и понятный ответ для клиента.\n",
        "Формат вывода:<ОТВЕТ:>'[Текст ответа]'<>'.\n",
        "\n",
        "4. Агент 4: Post-Processor (Обработчик Результатов)\n",
        "   - Задача: Проверить ответ и доставить его клиенту. Если ответ \"<ОТВЕТ:>Нет ответа<>\", запустить повторный анализ с Агента 1.\n",
        "   - Формат вывода:\n",
        "     Статус: [Ответ доставлен/Повторный анализ]\n",
        "\n",
        "ООБЯЗАТЕЛЬНЫЕ правила:\n",
        "- Последовательность выполнения: Агент 1 → Агент 2 → Агент 3 → Агент 4.\n",
        "- Если ответ не найден, система возвращается к Агенту 1 для повторного анализа ЕЩЕ 3 раза. ТОЛЬКО ПОСЛЕ ЭТОГО выдается \"<ОТВЕТ:>Нет ответа<>\".\n",
        "- Используются только теги из {uniq_tags}.\n",
        "- Ответы клиенту начинаются с \"<ОТВЕТ:>\" и заканчиваются \"<>\".\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "dPJJwjhAxTCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Выбор модели LLM и эмбеддингов\n",
        "output_file = 'submission.csv'  # @param {type:\"string\"}\n",
        "# @markdown ### Выбор модели LLM и эмбеддингов\n",
        "model_name = \"openrouter/mistralai/mistral-small-3.2-24b-instruct\"  # @param [\"openrouter/mistralai/mistral-small-3.2-24b-instruct\", \"openrouter/meta-llama/llama-3-70b-instruct\", \"openrouter/x-ai/grok-3-mini\", \"openrouter/google/gemma-3-27b-it\"] {type:\"string\"}\n",
        "model_embed = \"text-embedding-3-small\"  # @param [\"text-embedding-3-small\", \"text-embedding-ada-002\"] {type:\"string\"}"
      ],
      "metadata": {
        "id": "jdYodbZi-JUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBCYbA4VnbGg"
      },
      "outputs": [],
      "source": [
        "# Пути к файлам\n",
        "QUESTIONS_CSV = \"questions.csv\"\n",
        "SUBMISSION_CSV = \"submission.csv\"\n",
        "STATE_FILE = \"state.json\"  # Файл для сохранения состояния (индекс последнего обработанного вопроса)\n",
        "\n",
        "\n",
        "# Функция для разбиения текста на чанки\n",
        "def split_text(text: str, max_tokens: int = 8191) -> List[str]:\n",
        "    encoding = get_encoding(\"p50k_base\")\n",
        "    tokens = encoding.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_tokens):\n",
        "        chunk = tokens[i:i + max_tokens]\n",
        "        chunks.append(encoding.decode(chunk))\n",
        "    return chunks\n",
        "\n",
        "# Асинхронная функция для получения эмбеддингов\n",
        "async def get_embeddings(texts: List[str], embedding_model_name: str) -> List[List[float]]:\n",
        "    client = openai.AsyncOpenAI(\n",
        "    base_url=\"https://ai-for-finance-hack.up.railway.app/\",\n",
        "    api_key = userdata.get('TEXT_EMBER')\n",
        "    )\n",
        "    all_embeddings = []\n",
        "    # Пакет из 20 блоков * ~ 8191 токен/блок = ~160 тыс. токенов, чтобы уложиться в лимит с запасом.\n",
        "    batch_size = 20\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Getting embeddings in batches\"):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        try:\n",
        "            response = await client.embeddings.create(\n",
        "                model=embedding_model_name,\n",
        "                input=batch_texts,\n",
        "            )\n",
        "            all_embeddings.extend([item.embedding for item in response.data])\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при получении эмбеддингов для батча: {e}\")\n",
        "            return []\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_model = OpenAIEmbeddings(\n",
        "    openai_api_base=\"https://ai-for-finance-hack.up.railway.app/\",\n",
        "    openai_api_key=userdata.get('TEXT_EMBER'),\n",
        "    model=model_embed,\n",
        "    chunk_size=100 # Выбираем  размер блока, чтобы соблюсти ограничение на токены API\n",
        ")\n",
        "\n",
        "# Загружаем данные для контекста\n",
        "df = pd.read_csv(\"train_data.csv\")\n",
        "\n",
        "faiss_index_dir = \"faiss_index_storage\" # Каталог для LangChain FAISS index\n",
        "\n",
        "# Загружаем или создаём FAISS-индекс\n",
        "if Path(faiss_index_dir).exists(): # проверяем наличие католога\n",
        "    print(\"Грузим LangChain FAISS index...\")\n",
        "    db = FAISS.load_local(\n",
        "        faiss_index_dir,\n",
        "        embeddings_model,\n",
        "        index_name=\"faiss_index\",\n",
        "        allow_dangerous_deserialization=True\n",
        "    )\n",
        "    index = db.index\n",
        "    chunked_texts = [doc.page_content for doc in db.docstore._dict.values()] # Добавляем чанки\n",
        "\n",
        "else:\n",
        "    print(\"Создаем LangChain FAISS index...\")\n",
        "    # Получаем эмбеддинги для всех текстов в df\n",
        "    all_texts = df[\"text\"].tolist()\n",
        "    documents = []\n",
        "    chunked_texts = [] # Список для чанков\n",
        "    for i, text in enumerate(all_texts):\n",
        "        # Разбиваем тексты а чанки\n",
        "        chunks = split_text(text, max_tokens=8191)\n",
        "        chunked_texts.extend(chunks) # Добавляем чанки в список чанков\n",
        "        for j, chunk in enumerate(chunks):\n",
        "            # Создаем  объект документа для чанков\n",
        "            documents.append(Document(page_content=chunk, metadata={\"source\": f\"doc_{i}_chunk_{j}\"}))\n",
        "\n",
        "    # Создание векторного хранилища FAISS на основе документов\n",
        "    db = FAISS.from_documents(documents, embeddings_model)\n",
        "    # Сохраняем индекс FAISS\n",
        "    db.save_local(faiss_index_dir, index_name=\"faiss_index\")\n",
        "\n",
        "    index = db.index"
      ],
      "metadata": {
        "id": "BWdOmiT1XhtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Асинхронная функция для обработки одного вопроса\n",
        "async def process_question(\n",
        "    question: str,\n",
        "    index: faiss.Index,\n",
        "    df: pd.DataFrame,\n",
        "    chunked_texts: List[str], # Добавляем chunked_texts\n",
        "    k: int = 4\n",
        ") -> Optional[str]:\n",
        "    # Инициализация клиента OpenAI\n",
        "    chat_client = openai.AsyncOpenAI(\n",
        "        base_url=\"https://ai-for-finance-hack.up.railway.app/\",\n",
        "        api_key = userdata.get('OpenR-LLM')\n",
        "    )\n",
        "    try:\n",
        "        # Поиск релевантных ответов\n",
        "        query_embedding = await get_embeddings([question], model_embed)\n",
        "        if not query_embedding:\n",
        "            return None\n",
        "\n",
        "        distances, indices = index.search(np.array(query_embedding), k)\n",
        "        # Извлекем тексты, по индексам из chunked_texts\n",
        "        relevant_texts = [chunked_texts[idx] for idx in indices[0]]\n",
        "\n",
        "        # Генерация ответа\n",
        "        promt = f\"\"\"\n",
        "        Контекст:\n",
        "        {\" \".join(relevant_texts)}\n",
        "        Вопрос: {question}\n",
        "        Ответь на вопрос клиента, используя только предоставленный контекст.\n",
        "        Начни ответ с <ОТВЕТ:> и заверши <>.ЕМБЕДДИНГ\n",
        "        \"\"\"\n",
        "        response = await chat_client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {   \"role\": \"system\",\"content\":f\"{system_promt}\",\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [{\"type\": \"text\",\n",
        "                                 \"text\": f\"{user_promt}: {promt}\"\n",
        "                                }\n",
        "                               ]\n",
        "                }\n",
        "                     ],\n",
        "            seed=3111696,\n",
        "            temperature=0.25,\n",
        "            max_tokens=8191\n",
        "        )\n",
        "        answer = response.choices[0].message.content\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при обработке вопроса: {e}\")\n",
        "        return None\n",
        "\n",
        "# Асинхронная функция для записи ответа в submission.csv\n",
        "async def write_answer(\n",
        "    question_id: int,\n",
        "    question: str,\n",
        "    answer: str,\n",
        "    submission_file: str,\n",
        "    mode: str = \"a\"\n",
        ") -> None:\n",
        "    data = {\"id\": question_id, \"question\": question, \"answer\": answer}\n",
        "    df = pd.DataFrame([data])\n",
        "    header = not Path(submission_file).exists() or mode == \"w\"\n",
        "    df.to_csv(submission_file, mode=mode, index=False, header=header, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "L_BozAkgSgiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обработка всего массива вопросов с продолжением с места окончания в случае прерывания"
      ],
      "metadata": {
        "id": "Pyl0aAIBXjKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Асинхронная функция для обработки всех вопросов\n",
        "async def process_questions(\n",
        "    questions_file: str,\n",
        "    submission_file: str,\n",
        "    state_file: str,\n",
        "    index: faiss.Index,\n",
        "    df: pd.DataFrame,\n",
        "    chunked_texts: List[str]\n",
        ") -> None:\n",
        "    # Загружаем вопросы\n",
        "    questions_df = pd.read_csv(questions_file)\n",
        "\n",
        "    # Загружаем состояние (индекс последнего обработанного вопроса)\n",
        "    last_processed_index = 0\n",
        "    if Path(state_file).exists():\n",
        "        with open(state_file, \"r\") as f:\n",
        "            state = json.load(f)\n",
        "            last_processed_index = state.get(\"last_processed_index\", 0)\n",
        "\n",
        "    # Обрабатываем вопросы, начиная с последнего сохранённого индекса\n",
        "    for idx, row in questions_df.iterrows():\n",
        "        if idx < last_processed_index:\n",
        "            continue\n",
        "\n",
        "        question = row[\"Вопрос\"]\n",
        "        question_id = row.get(\"id\", idx)\n",
        "\n",
        "        print(f\"Обработка вопроса {idx}: {question}\")\n",
        "\n",
        "        # Обрабатываем вопрос\n",
        "        answer = await process_question(question, index, df, chunked_texts) # Pass chunked_texts\n",
        "        if answer:\n",
        "\n",
        "            await write_answer(question_id, question, answer, submission_file)\n",
        "\n",
        "            # Сохраняем состояние\n",
        "            with open(state_file, \"w\") as f:\n",
        "                json.dump({\"last_processed_index\": idx}, f)\n",
        "\n",
        "# Основная асинхронная функция\n",
        "async def main():\n",
        "\n",
        "    # Обрабатываем вопросы\n",
        "    await process_questions(QUESTIONS_CSV, SUBMISSION_CSV, STATE_FILE, index, df, chunked_texts)\n"
      ],
      "metadata": {
        "id": "3VZfZbK5T0Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запуск  программы с обработкой всех вопросов из questions.scv"
      ],
      "metadata": {
        "id": "q2PVi6uDbzms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "id": "prmJkl5x0Z18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поиск соответствий в базе"
      ],
      "metadata": {
        "id": "ttEd_bx6XQJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Введите вопрос:\n",
        "query = 'Какие риски инвестиций в криптовалюту'  # @param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "ZWjVsPCfaGv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_and_scores = db.similarity_search_with_score(query, k=4)\n",
        "\n",
        "# Вывод результатов\n",
        "for doc, score in docs_and_scores:\n",
        "    print(f\"Score: {score}\")\n",
        "    print(f\"Text: {doc.page_content}\")\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "id": "JBNRMfqOS7rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "7380a343",
        "outputId": "c6e80378-1584-4289-a083-f13f48e5db72"
      },
      "source": [
        "# Ответ на вопрос\n",
        "await process_question(query,index, df,chunked_texts, 4)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Getting embeddings in batches: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<ОТВЕТ:>\\nИнвестиции в криптовалюту связаны с рядом значительных рисков, которые важно учитывать:\\n\\n1. **Высокий волатильность**: Цены на криптовалюты могут резко меняться, что делает их инвестиции особенно рискованными. Это отличает их от более стабильных активов, таких как облигации или вклады.\\n\\n2. **Отсутствие государственной страховки**: В отличие от банковских вкладов, инвестиции в криптовалюту не защищены системой страхования вкладов. В случае потери средств (например, из-за взлома кошелька или банкротства биржи) вернуть их будет сложно.\\n\\n3. **Риск мошенничества**: Криптовалютный рынок привлекает мошенников, предлагающих сомнительные проекты, пирамиды или фишинговые схемы. Важно внимательно проверять любые предложения.\\n\\n4. **Регуляторные риски**: В разных странах законодательство о криптовалютах может меняться, что влияет на их ликвидность и доступность. Например, в некоторых юрисдикциях могут вводиться ограничения или запреты.\\n\\n5. **Технические риски**: Потеря доступа к кошельку (например, из-за утери пароля или приватного ключа) может привести к невозможности воспользоваться средствами.\\n\\n6. **Риск ликвидности**: Некоторые криптовалюты могут быть сложно продать по выгодной цене, особенно в условиях резкого падения спроса.\\n\\nРекомендуется инвестировать в криптовалюту только те средства, которые вы готовы потерять, и не использовать для этого последние сбережения. Также важно диверсифицировать инвестиции и следить за новостями рынка.\\n<>\\n\\n**Примечание**: В предоставленном контексте нет прямой информации о рисках криптовалют, но общие принципы инвестирования (например, отсутствие страховки, волатильность) применимы и к этому активу.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bQO_Ph-m0MZ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}